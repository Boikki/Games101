### 深度解析 HomeWork 5的框架结构



##### main.cpp

```c++
    Scene scene(1280, 960);
```

New 一个scene对象出来 

```c++
class Scene
{
public:
    // setting up options
    int width = 1280;
    int height = 960;
    double fov = 90;
    Vector3f backgroundColor = Vector3f(0.235294, 0.67451, 0.843137);
    int maxDepth = 5;
    float epsilon = 0.00001;

    Scene(int w, int h) : width(w), height(h)
    {}

    void Add(std::unique_ptr<Object> object) { objects.push_back(std::move(object)); }
    void Add(std::unique_ptr<Light> light) { lights.push_back(std::move(light)); }
	// [[nodiscard]] 是 C++11 中引入的一个属性（attribute）
    // 用于指示编译器在调用函数后要检查其返回值，并在没有使用该返回值时发出警告。
    // 该属性通常用于指示函数返回的结果具有重要性，而且应该被程序员正确处理，否则可能会导致意外的行为或错误。
    // 也就是要调用get_objects 必须制定返回值为vector<xxx> 而不能直接调用
    [[nodiscard]] const std::vector<std::unique_ptr<Object> >& get_objects() const { return objects; }
    [[nodiscard]] const std::vector<std::unique_ptr<Light> >&  get_lights() const { return lights; }

private:
    // creating the scene (adding objects and lights)
    std::vector<std::unique_ptr<Object> > objects;
    std::vector<std::unique_ptr<Light> > lights;
};

```

最后两行是添加物体和光线进场景

#### 进入Objects.hpp

```c++
class Object
{
public:
    Object()
        // 默认材质是粗糙的
        // 定义了物体的若干属性：
        
       
        : materialType(DIFFUSE_AND_GLOSSY)
            // ior-反射率 
        , ior(1.3)
             // Kd Ks是Blin-Phong模型的漫反射和镜面反射系数
        , Kd(0.8)
        , Ks(0.2)
            // 漫反射颜色（物体的基本颜色）
        , diffuseColor(0.2)
            // 高光反射指数
        , specularExponent(25)
    {}
	// 纯虚函数 子类必须全部重写
    virtual ~Object() = default;

    virtual bool intersect(const Vector3f&, const Vector3f&, float&, uint32_t&, Vector2f&) const = 0;

    virtual void getSurfaceProperties(const Vector3f&, const Vector3f&, const uint32_t&, const Vector2f&, Vector3f&, Vector2f&) const = 0;

    virtual Vector3f evalDiffuseColor(const Vector2f&) const
    {
        return diffuseColor;
    }

    // material properties
    MaterialType materialType;
    float ior;
    float Kd, Ks;
    Vector3f diffuseColor;
    float specularExponent;
};

```



##### 回到main.cpp

之后开始向场景中新增球，三角形和光线

```c++
    auto sph1 = std::make_unique<Sphere>(Vector3f(-1, 0, -12), 2);
```

#### 进入Sphere.hpp



```c++
class Sphere : public Object
{
public:
    // 重心，半径
    Sphere(const Vector3f& c, const float& r)
        : center(c)
        , radius(r)
        , radius2(r * r)
    {}
	// 判断光线和球体的交点
    bool intersect(const Vector3f& orig, const Vector3f& dir, float& tnear, uint32_t&, Vector2f&) const override
    {
        // analytic solution
        Vector3f L = orig - center;
        float a = dotProduct(dir, dir);
        float b = 2 * dotProduct(dir, L);
        float c = dotProduct(L, L) - radius2;
        float t0, t1;
        if (!solveQuadratic(a, b, c, t0, t1))
            return false;
        if (t0 < 0)
            t0 = t1;
        if (t0 < 0)
            return false;
        tnear = t0;

        return true;
    }

    void getSurfaceProperties(const Vector3f& P, const Vector3f&, const uint32_t&, const Vector2f&,
                              Vector3f& N, Vector2f&) const override
    {
        N = normalize(P - center);
    }

    Vector3f center;
    float radius, radius2;
};

```



##### 解读solveQuadratic

光线方程：

$P(t) = P_0 + t\vec{d}$

其中，$P(t)$ 表示光线上距离起点 $P_0$ 为 $t$ 的点。

球体方程：

$(P - C) \cdot (P - C) = r^2$

其中，$\cdot$ 表示向量点乘。

将光线方程代入球体方程中，得到：



$(P_0 + t\vec{d} - C) \cdot (P_0 + t\vec{d} - C) = r^2$

这是一个关于 $t$ 的二次方程，可以通过求解 $t$ 来得到交点。具体地，可以将上式展开，化简得：

$t^2\vec{d}\cdot\vec{d} + 2t(\vec{d}\cdot(P_0-C)) + (P_0-C)\cdot(P_0-C)-r^2 = 0$

且

```c++
		float a = dotProduct(dir, dir);
        float b = 2 * dotProduct(dir, L);
        float c = dotProduct(L, L) - radius2;
```

由代码得

$\vec{L} = P_{0} - C \\ a = \vec{d} \cdot  \vec{d} = |d|^2 \\ b = 2 \vec{d} \cdot \vec{L} \\ c = \vec{L} \cdot \vec{L} - r^2 = |L|^2 - r^2$

根据求根定理求根就好了！

`solveQuadratic`就是根据传入的abc带出两个交点来



##### 回到main

move将sph1转换成右值（避免了一次拷贝构造）

```c++
    scene.Add(std::move(sph1));
```



将MeshTriangle添加到scene中

```c++
class MeshTriangle : public Object
{
public:
    MeshTriangle(const Vector3f* verts, const uint32_t* vertsIndex, const uint32_t& numTris, const Vector2f* st)
    { }

    bool intersect(const Vector3f& orig, const Vector3f& dir, float& tnear, uint32_t& index,
                   Vector2f& uv) const override
    {
        bool intersect = false;
        for (uint32_t k = 0; k < numTriangles; ++k)
        {
            // vertex[Index]是按顺序存储的numTriangles 个三角形的顶点信息
            // 所以取第k个就是k * 3 + x
            const Vector3f& v0 = vertices[vertexIndex[k * 3]];
            const Vector3f& v1 = vertices[vertexIndex[k * 3 + 1]];
            const Vector3f& v2 = vertices[vertexIndex[k * 3 + 2]];
            float t, u, v;
            // rayTriangleIntersect是上次实现的
            // 根据重心坐标求光线和三角形交点
            if (rayTriangleIntersect(v0, v1, v2, orig, dir, t, u, v) && t < tnear)
            {
                // 如果有交点就更新
                tnear = t;
                uv.x = u;
                uv.y = v;
                index = k;
                intersect |= true;
            }
        }

        return intersect;
    }
	// 计算交点所在平面的法向量，以及交点的st坐标(只有三角形有)。
    // 最后两个是传出参数
    void getSurfaceProperties(const Vector3f&, const Vector3f&, const uint32_t& index, const Vector2f& uv, Vector3f& N, Vector2f& st) const override
    {
        const Vector3f& v0 = vertices[vertexIndex[index * 3]];
        const Vector3f& v1 = vertices[vertexIndex[index * 3 + 1]];
        const Vector3f& v2 = vertices[vertexIndex[index * 3 + 2]];
        Vector3f e0 = normalize(v1 - v0);
        Vector3f e1 = normalize(v2 - v1);
        N = normalize(crossProduct(e0, e1));
        const Vector2f& st0 = stCoordinates[vertexIndex[index * 3]];
        const Vector2f& st1 = stCoordinates[vertexIndex[index * 3 + 1]];
        const Vector2f& st2 = stCoordinates[vertexIndex[index * 3 + 2]];
        st = st0 * (1 - uv.x - uv.y) + st1 * uv.x + st2 * uv.y;
    }
	
    // 渲染出地板的效果
    采用了一个简单的格子纹理模式，其中将纹理坐标乘以一个比例因子 $scale$，
然后通过模 1 的操作将其缩放到 $[0, 1]$ 的范围内，从而将坐标映射到一个二维的格子中。
然后，函数使用逻辑异或操作符 ^ 将两个坐标的结果进行异或运算，
生成一个 0 或 1 的值，用于确定当前纹理坐标是否在格子的黑色部分或白色部分。
最后，函数使用线性插值（lerp）将格子颜色中的黑色部分（(0.815, 0.235, 0.031)）
和白色部分（(0.937, 0.937, 0.231)）进行混合，得到最终的漫反射颜色。
    Vector3f evalDiffuseColor(const Vector2f& st) const override
    {
        float scale = 5;
        float pattern = (fmodf(st.x * scale, 1) > 0.5) ^ (fmodf(st.y * scale, 1) > 0.5);
        return lerp(Vector3f(0.815, 0.235, 0.031), Vector3f(0.937, 0.937, 0.231), pattern);
    }
};

```









下面是自行实现的三角形求交点

求出向量来带公式即可

<img src="https://img-blog.csdnimg.cn/bca9335c1fdb469d80c2d2830f9005cd.png" alt="img" style="zoom:67%;" />

```c++
bool rayTriangleIntersect(const Vector3f& v0, const Vector3f& v1, const Vector3f& v2, const Vector3f& orig,
                          const Vector3f& dir, float& tnear, float& u, float& v)
{
    // TODO: Implement this function that tests whether the triangle
    // that's specified bt v0, v1 and v2 intersects with the ray (whose
    // origin is *orig* and direction is *dir*)
    // Also don't forget to update tnear, u and v.
    // 三角形的顶点是v0, v1. v2
    // Ray 是orig --dir--> 求他们的交点在不在三角形内
    // 在的话tnear, u, v, 1 - u - v 都属于[0, 1]
    Vector3f e1 = v1 - v0;
    Vector3f e2 = v2 - v0;
    Vector3f s = orig - v0;
    Vector3f s1 = crossProduct(dir, e2);
    Vector3f s2 = crossProduct(s, e1);

    auto frac = dotProduct(s1, e1);
    tnear = dotProduct(s2, e2) / frac;
    u = dotProduct(s1, s);
    v = dotProduct(s2, dir);
    // 注意tnear 必须 > 0也就是三角形必须在射线传播的方向上
    // 这个和包围盒还是不一样的！
    if (tnear >= 0 && u >= 0 && v >= 0 && 1 - u - v >= 0) {
         return true;
     } else return false;
```



---



##### 最后开始渲染场景

在 Render 方法中，首先定义了尺度scale与宽高比imageAspectRatio，以及相机位置在(0, 0, 0)

>眼睛在（0，0，0）的坐标原点处，屏幕的像素是1280*960，屏幕的fov是90度，宽高比也是已知的。
>
>但是只知道这些信息我根本不知道屏幕在哪里，一个像素到底表示多大的范围，离摄像机（眼睛）距离到底多远。
>
>通过下面的代码
>
>```c++
>            Vector3f dir = normalize(Vector3f(x, y, -1)); // Don't forget to normalize this direction!
>            framebuffer[m++] = castRay(eye_pos, dir, scene, 0);
>```
>
>dir是光线的方向，光源又在原点出，所以坐标（x, y,-1）一定就是我们要计算的像素位置，可以发现**每个像素都在深度z=-1的位置上**。
>
>所以我们立刻可以知道，屏幕离我们摄像机的距离就是-1，既然深度知道，fov知道，宽高比知道，而且我们默认屏幕于X轴和Y轴对称，我们立刻就可以计算出屏幕长宽，这样我们就知道每个像素点的具体坐标了（不要忘了像素点的坐标是像素的中心坐标（需要+0.5））。

```c++
std::vector<Vector3f> framebuffer(scene.width * scene.height);

float scale = std::tan(deg2rad(scene.fov * 0.5f));
float imageAspectRatio = scene.width / (float)scene.height;

// Use this variable as the eye position to start your rays.
Vector3f eye_pos(0);
int m = 0;
```









[参考](https://www.scratchapixel.com/lessons/3d-basic-rendering/ray-tracing-generating-camera-rays/generating-camera-rays.html)

然后对于每一个像素，将其从光栅化坐标映射回世界坐标

这就是需要我们自己实现的了

先放结论，要经历的步骤有：

>1. Normalized (to NDC [0, 1])
>2. Remapped (-1, 1)
>3. Mul the image aspect ratio
>4. Mul the tan(a / 2)

也就是将

Raster Space -> NDC Space -> Screen Space -> World Space

该点的坐标首先在光栅空间中表示（像素坐标加上偏移量0.5），

转换为NDC空间（坐标重新映射到范围[0,1]），

转换为屏幕空间（NDC坐标被重新映射到 [-1,1])

应用最终的(camera to world) 4x4 矩阵将屏幕空间中的坐标转换为世界空间。



为什么要变换到Word Space？因为World Space物体相对位置都是正确的

是没有 透视或者正交投影过的 Z轴都是正确的 才能做光线追踪！



<img src="/Users/renboyu/Library/Application Support/typora-user-images/image-20230503102805729.png" alt="image-20230503102805729" style="zoom:50%;" />

##### Camera Space

<img src="/Users/renboyu/Library/Application Support/typora-user-images/image-20230503103146519.png" alt="image-20230503103146519" style="zoom:50%;" />

一般来说相机是朝向 -z

此图展示的已经是相机空间的图像了 xy都是[-1, 1]

但是原始图像大小为 6x6 像素，

眼睛的默认位置为世界中心 (0, 0, 0)

图像平面距离原点（也就是eye_pos, camera_pos）恰好 1 个单位

##### Camera Space to NDC(which stands for Normalized Device Coordinates) Space

$\begin{array}{l}
PixelNDC_x = \dfrac{(Pixel_x + 0.5)}{ImageWidth},\\
PixelNDC_y = \dfrac{(Pixel_y + 0.5)}{ImageHeight}.
\end{array}$

为什么要加0.5的偏置？ 

因为一般来说ij表示的是像素左下角的点，做光线追踪时我们希望光线能穿过像素中心

现在得到$NDC_x, NDC_y \in [0, 1]$

##### NDC to Screen

$\begin{array}{l}
PixelScreen_x = 2 * {PixelNDC_x} - 1,\\
PixelScreen_y = 2 * {PixelNDC_y} - 1.
\end{array}$

但是这样y会有问题，属于[0, 0.5]的y会被映射成负的

所以取负数就好了

$PixelScreen_y = 1 - 2 * {PixelNDC_y}$

###### 宽高比

上面得到的都被放缩到了[0, 1]中

但是image的宽高并不是1:1的

还需要乘一个宽高比值$ImageAspectRatio = \frac{Width}{Height}$

最终得到

$\begin{array}{l}
ImageAspectRatio = \dfrac{ImageWidth}{ImageHeight},\\
PixelCamera_x = (2 * {PixelScreen_x} - 1) * {ImageAspectRatio},\\
PixelCamera_y = (1 - 2 * {PixelScreen_y}).
\end{array}$

###### 视野(很直观的东西 视野越大 tan越大 能看到的东西越多)

就相当于对image裁切了

![img](https://www.scratchapixel.com/images/ray-tracing-camera/camprofile.png?)

因为上文提到image放在了离camera 为1的位置

就是(0, 0, 1)

所以AB = 1

那么$||BC|| = tan(\dfrac{\alpha}{2})$

最后得到完整公式

$\begin{array}{l}
PixelCamera_x = (2 * {PixelScreen_x } - 1) * ImageAspectRatio * tan(\dfrac{\alpha}{2}),\\
PixelCamera_y = (1 - 2 * {PixelScreen_y }) * tan(\dfrac{\alpha}{2}).
\end{array}$



最后得到每个pixel的坐标(x, y, -1)

但是此时相机时固定在原点的

如果想获得任意位置相机观测的就做的简单的trans变换就可以啦

乘4 * 4的变换矩阵

Usually, this matrix is called the **camera-to-world matrix** (and its inverse is called the **world-to-camera matrix**).



##### 示例代码

可以看到和课堂的代码如出一辙

13-14 

```c++
void render(
    const Options &options,
    const std::vector<std::unique_ptr<Object>> &objects,
    const std::vector<std::unique_ptr<Light>> &lights)
{
    Matrix44f cameraToWorld;
    Vec3f *framebuffer = new Vec3f[options.width * options.height];
    Vec3f *pix = framebuffer;
    float scale = tan(deg2rad(options.fov * 0.5));
    float imageAspectRatio = options.width / (float)options.height;
    Vec3f orig;
    cameraToWorld.multVecMatrix(Vec3f(0), orig);
    // 循环遍历每一个pixel，并且计算每个pixel的光照
    for (uint32_t j = 0; j < options.height; ++j) {
        for (uint32_t i = 0; i < options.width; ++i) {
            // 一行实现上面所说的坐标转换
            float x = (2 * (i + 0.5) / (float)options.width - 1) * imageAspectRatio * scale;
            float y = (1 - 2 * (j + 0.5) / (float)options.height) * scale;
            Vec3f dir;
            // z = -1 也就是camera看向-z方向
            // 这里注意了 图形学中的camera不是小孔成像的原理！
            // 小孔成像的图像是颠倒的
            // 这里camera的胶片和image算是在同一侧 才能达到放缩
            // 最后乘上4 * 4变换矩阵
            cameraToWorld.multDirMatrix(Vec3f(x, y, -1), dir);
            dir.normalize();
            *(pix++) = castRay(orig, dir, objects, lights, options, 0);
        }
    }

    // Save result to a PPM image (keep these flags if you compile under Windows)
    std::ofstream ofs("./out.ppm", std::ios::out | std::ios::binary);
    ofs << "P6\n" << options.width << " " << options.height << "\n255\n";
    for (uint32_t i = 0; i < options.height * options.width; ++i) {
        char r = (char)(255 * clamp(0, 1, framebuffer[i].x));
        char g = (char)(255 * clamp(0, 1, framebuffer[i].y));
        char b = (char)(255 * clamp(0, 1, framebuffer[i].z));
        ofs << r << g << b;
    }

    ofs.close();

    delete [] framebuffer;
}
```





```c++
    for (int j = 0; j < scene.height; ++j)
    {
        for (int i = 0; i < scene.width; ++i)
        {
            // generate primary ray direction
            // TODO: Find the x and y positions of the current pixel to get the direction
            // vector that passes through it.
            // Also, don't forget to multiply both of them with the variable *scale*, and
            // x (horizontal) variable with the *imageAspectRatio*            
            // x = scale * imageAspectRatio * (2 * (i + 0.5) / (float)scene.width - 1);
            // y = scale * (1 - 2 * (j + 0.5) / (float)scene.height);
            float x = (2 * (((float)i + 0.5) / scene.width - 1)) * imageAspectRatio * scale;
            float y = (1.0 - 2 * ((float)j + 0.5) / scene.height) * scale;

            Vector3f dir = normalize(Vector3f(x, y, -1)); // Don't forget to normalize this direction!
            framebuffer[m++] = castRay(eye_pos, dir, scene, 0);
        }
        UpdateProgress(j / (float)scene.height);
    }

    // save framebuffer to file
    FILE* fp = fopen("binary.ppm", "wb");
    (void)fprintf(fp, "P6\n%d %d\n255\n", scene.width, scene.height);
    for (auto i = 0; i < scene.height * scene.width; ++i) {
        static unsigned char color[3];
        color[0] = (unsigned char)255 * clamp(0, 1, framebuffer[i].x);
        color[1] = (unsigned char)255 * clamp(0, 1, framebuffer[i].y);
        color[2] = (unsigned char)255 * clamp(0, 1, framebuffer[i].z);
        fwrite(color, 1, 3, fp);
    }
    fclose(fp);    
}

```





----



##### 下面进入castRay 光线求交并且计算颜色



先看官方给的注释



>
>
>// [comment]
>// Implementation of the Whitted-style light transport algorithm (E [S*] (D|G) L)
>//
>// This function is the function that compute the color at the intersection point
>// of a ray defined by a position and a direction. Note that thus function is recursive (it calls itself).
>// 当材质是反射/折射的时候 就递归下去生成两条新的光线
>
>// 当材质是透明的时候 用fresnel equations 计算表面
>
>// 漫反射就用经典Phong光照（注意不是Blin-Phong着色）
>
>// If the material of the intersected object is either reflective or reflective and refractive,
>// then we compute the reflection/refraction direction and cast two new rays into the scene
>// by calling the castRay() function recursively. When the surface is transparent, we mix
>// the reflection and refraction color using the result of the fresnel equations (it computes
>// the amount of reflection and refraction depending on the surface normal, incident view direction
>// and surface refractive index).
>//
>// If the surface is diffuse/glossy we use the Phong illumation model to compute the color
>// at the intersection point.
>// [/comment]



一开始对 `depth` 进行比较，这里应该是对光线折射次数的定义

（因为Whitted风格的光线追踪考虑光线是不断反射的），

在这里场景中的光线折射次数限定为 5 次(scene.maxDepth = 5)

*也就是超过反射5次的光线就不再参与光照了，这也就是为什么Whitted风格不能称之为全局光照的原因，其实主要原因还是没考虑其他物体的光照*

*这不就相当于渲染方程的前5项么*

$\mathbf{L} = \mathbf{E} + k\mathbf{E} + k ^ 2\mathbf{E}+...$

*但是在后续的Ray Pathing中会有不同的终止条件：RR*

然后给颜色初始化为背景颜色

```c++
Vector3f castRay(const Vector3f &orig, const Vector3f &dir, const Scene& scene, int depth) {
    if (depth > scene.maxDepth) {
        return Vector3f(0.0,0.0,0.0);
    }

```

然后使用 `trace` 判断光线是否与场景中的物体有交点，有交点执行之后的代码，没有交点直接返回背景颜色

`trace` 就是判断当前射线是否与空间中的物体有交点，如果有交点，返回最近的交点。

`trace`内部也使用了`obj->insterect`来检测交点

>在C++17中，`std::optional`是一个新的标准库类型，它表示一个可选的值，即一个值可以存在也可以不存在。
>
>`std::optional`可以用于取代C++中常用的nullptr或无效值的概念，从而使代码更加安全和易于理解。
>
>需要注意的是，`std::optional`并不是一个指针类型，因此无需使用解引用运算符`*`和箭头运算符`->`来访问其值。
>
>相反，可以使用`value()`函数来获取值，该函数将抛出异常如果对象为空。
>
>此外，可以使用`value_or()`函数来获取对象的值，如果对象为空，则返回一个默认值。

```c++
    Vector3f hitColor = scene.backgroundColor;
    if (auto payload = trace(orig, dir, scene.get_objects()); payload)
        
std::optional<hit_payload> trace(const Vector3f &orig, const Vector3f &dir, const std::vector<std::unique_ptr<Object> > &objects) {
       
    return payload;
}

```



因为`obj->insterect`是会更新相交时间t的，所以可以用`obj->tNear`来求得交点

```c++
Vector3f hitPoint = orig + dir * payload->tNear;
```

然后使用 `getSurfaceProperties` 计算交点所在平面的法向量，以及交点的st坐标(只有三角形有)。



```c++
Vector3f N; // normal
Vector2f st; // st coordinates
payload->hit_obj->getSurfaceProperties(hitPoint, dir, payload->index, payload->uv, N, st);    

// Shpere 交点 - 球心就是法向量
void getSurfaceProperties(const Vector3f& P, const Vector3f&, const uint32_t&, const Vector2f&,
                              Vector3f& N, Vector2f&) const override
    {
        N = normalize(P - center);
    }

// Triangle 多一个计算st坐标
// uv是传入参数 实际上就是通过插值计算三角形内任意一点对应texture的坐标
// st0, st1, st2是三个顶点的st（也叫uv）坐标
        const Vector2f& st0 = stCoordinates[vertexIndex[index * 3]];
        const Vector2f& st1 = stCoordinates[vertexIndex[index * 3 + 1]];
        const Vector2f& st2 = stCoordinates[vertexIndex[index * 3 + 2]];
        st = st0 * (1 - uv.x - uv.y) + st1 * uv.x + st2 * uv.y;

```

>GPT说：
>
>计算机图形学中，三角形的纹理坐标（texture coordinates）通常用ST坐标（或UV坐标）来表示。ST坐标是二维的，它们定义了纹理图像上的一个位置，用于确定该位置处的纹理像素与三角形表面上对应的点之间的映射关系。
>
>ST坐标的取值范围通常是[0, 1]。其中，S坐标代表了纹理图像中水平方向上的位置，T坐标代表了垂直方向上的位置。在OpenGL中，ST坐标的原点通常在纹理图像的左下角。
>
>三角形的ST坐标可以通过插值计算得出，方法与插值计算三角形的顶点颜色类似。假设三角形的三个顶点的纹理坐标分别为$(s_1, t_1), (s_2, t_2), (s_3, t_3)$，则对于三角形内任意一点$(s, t)$，其ST坐标可以如下计算：
>
>$s = w_1s_1 + w_2s_2 + w_3s_3 \\ t = w_1t_1 + w_2t_2 + w_3t_3 \\ w_1 + w_2 + w_3 = 1$
>
>其中，$w_1, w_2, w_3$是点$(s, t)$在三角形内的重心坐标（barycentric coordinates），可以通过三角形顶点的屏幕坐标计算得出。重心坐标是一个用于表示点在三角形内位置的坐标系，它满足下列性质：
>
>1. 重心坐标的每个分量都是非负数；
>2. 重心坐标的所有分量之和为1；
>3. 如果一个点在三角形的边上，那么它的某个重心坐标分量为0；
>4. 如果一个点在三角形的某个顶点上，那么对应的重心坐标分量为1，其余分量为0。
>
>插值计算ST坐标可以使用上述公式，这样就可以在渲染三角形时进行纹理映射，从而让三角形表面显示出正确的纹理效果。



然后就是根据触碰到的物体的材质，执行不同的算法获得对应颜色



```
switch (payload->hit_obj->materialType)
```

##### case REFLECTION_AND_REFRACTION

字面意思，既有反射也有折射，首先使用`reflect`函数计算反射方向，`reflect` 函数如下

```c++
Vector3f reflect(const Vector3f &I, const Vector3f &N) {
    return I - 2 * dotProduct(I, N) * N;
}
```

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMwLmNuYmxvZ3MuY29tL2Jsb2cvNjQyNTcvMjAxMzAyLzIxMTcxMzU5LWZmODU2ZWZlZmQ5YjRmZmRhYjI1ZjBmYzc1ZDBmNGUzLmpwZw?x-oss-process=image/format,png" alt="img" style="zoom:67%;" />

>平移$R$到$I$的起点位置
>
>发现I R N组成了等腰三角形
>
>且$R = I + 2S \ \ \ \ (1)$
>
>S实际上就是-I在N上的投影了
>
>$S = -\frac{I \cdot N}{|N|^2} * N \\  = -I \cdot N * N$
>
>带入上式
>
>有$R = I - 2 * I \cdot N * N$

然后使用`refract`函数计算折射方向，`refract` 函数如下(using Sell's law)

函数时分了两种情况

1. ray outside：cosi是正的且cosi = -NI
2. Ray inside：反转折射索引？并且反转N

```c++
Vector3f refract(const Vector3f &I, const Vector3f &N, const float &ior) {
    float cosi = clamp(-1, 1, dotProduct(I, N));
    float etai = 1, etat = ior;
    Vector3f n = N;
    if (cosi < 0) { cosi = -cosi; } else { std::swap(etai, etat); n= -N; }
    float eta = etai / etat;
    float k = 1 - eta * eta * (1 - cosi * cosi);
    return k < 0 ? 0 : eta * I + (eta * cosi - sqrtf(k)) * n;
}
```



然后计算反射光线与折射光线的起始点，这里为什么要 $± N ∗ \mathbf{\epsilon}  $，

是因为之后可能会继续判断射线是否与物体有接触，所以要加上或减取一个很小的值，防止有接触到当前点。

```c++
                Vector3f reflectionRayOrig = (dotProduct(reflectionDirection, N) < 0) ?
                                             hitPoint - N * scene.epsilon :
                                             hitPoint + N * scene.epsilon;
                Vector3f refractionRayOrig = (dotProduct(refractionDirection, N) < 0) ?
                                             hitPoint - N * scene.epsilon :
                                             hitPoint + N * scene.epsilon;
```

然后因为在 Whitted风格的光线追踪模型中 `REFLECTION_AND_REFRACTION` 的光完全由反射和折射光决定，

所以之后再用 `castRay` 计算出反射颜色`reflectionColor`与折射颜色`refractionColor`。

```c++
Vector3f reflectionColor = castRay(reflectionRayOrig, reflectionDirection, scene, depth + 1);
Vector3f refractionColor = castRay(refractionRayOrig, refractionDirection, scene, depth + 1);
```

那么涉及到反射与折射，使用菲涅尔项计算出对应的反射比例`kr`，然后加权出对应的颜色 `hitColor`

```c++
float kr = fresnel(dir, N, payload->hit_obj->ior);
hitColor = reflectionColor * kr + refractionColor * (1 - kr);
break;
```



##### REFLECTION

这个与上面 `REFLECTION_AND_REFRACTION`类似，就是没有折射项。

----



##### default(DIFFUSE_AND_GLOSSY)





默认项就是漫反射类型的材质，其使用 Phong 模型计算对应的漫反射项与镜面反射项。其公式如下，这里不使用$L_a$

$I = I_{ambient} + I_{diffuse} + I_{specular}$

$I_{diffuse} = k_d \cdot c_d \cdot \max(0, \vec{N} \cdot \vec{L})$

$I_{specular} = k_s \cdot c_s \cdot \max(0, \vec{R} \cdot \vec{V})^{k_e}$



首先判断当前点与光线的连线是否与物体接触(即是否被遮挡住)，如果被遮挡住，其 漫反射项 `lightAmt` 就是 0



```c++
                    Vector3f lightDir = light->position - hitPoint;
                    // square of the distance between hitPoint and the light
                    float lightDistance2 = dotProduct(lightDir, lightDir);
                    lightDir = normalize(lightDir);
                    float LdotN = std::max(0.f, dotProduct(lightDir, N));
                    // is the point in shadow,
                    // and is the nearest occluding object closer to the object than the light itself?
					// trace 的返回值是 optional 没有交点就返回nullptr了估计
                    auto shadow_res = trace(shadowPointOrig, lightDir, scene.get_objects());
					// 如果和物体有交点并且交点小于光源到当前点的距离 那不就是被遮住了
					// 就是在shadow中 不用更新Amt了
                    bool inShadow = shadow_res && (shadow_res->tNear * shadow_res->tNear < lightDistance2);

                    lightAmt += inShadow ? 0 : light->intensity * LdotN;
```



下面计算镜面反射项$I_s$



```c++
Vector3f reflectionDirection = reflect(-lightDir, N);

specularColor += powf(std::max(0.f, -dotProduct(reflectionDirection, dir)),
payload->hit_obj->specularExponent) * light->intensity;
```





当前点的 `hitColor` 就是漫反射项*Kd+镜面反射项*Ks，这里使用 `evalDiffuseColor(st)` 渲染出地板的效果

返回值是地板颜色 对应上述公式的$c_d\ c_s$

>采用了一个简单的格子纹理模式，其中将纹理坐标乘以一个比例因子 $scale$，
>
>然后通过模 1 的操作将其缩放到 $[0, 1]$ 的范围内，从而将坐标映射到一个二维的格子中。
>
>然后，函数使用逻辑异或操作符 ^ 将两个坐标的结果进行异或运算，
>
>生成一个 0 或 1 的值，用于确定当前纹理坐标是否在格子的黑色部分或白色部分。
>
>最后，函数使用线性插值（lerp）将格子颜色中的黑色部分（(0.815, 0.235, 0.031)）
>
>和白色部分（(0.937, 0.937, 0.231)）进行混合，得到最终的漫反射颜色。

```c++
    Vector3f evalDiffuseColor(const Vector2f& st) const override
    {
        float scale = 5;
        float pattern = (fmodf(st.x * scale, 1) > 0.5) ^ (fmodf(st.y * scale, 1) > 0.5);
        return lerp(Vector3f(0.815, 0.235, 0.031), Vector3f(0.937, 0.937, 0.231), pattern);
    }

```

>##### SUMMARY
>
>1. 当光线打在透明物体和镜面物体上时，由于这两种物体没有颜色，我们看到的颜色都是光线穿过它或者被反射到其他物体上的颜色，所以需要再次调用castRay函数，看一下下一次的反射或者折射的光线能不能打到有颜色的物体上。
>
>2. 当光线打在粗糙的物体上，我们需要在光线和物体的交点上（hitpoint），看一下能不能看到光源（从hitpoint发射一条连接光源的线，看一下和场景里面的物体有没有交点，如果**没有**或者**有交点但是交点在光线身后**，则可以看到光源，即被光线照射）。如果能看到光源，则用Phong模型计算光照颜色，反之则在阴影内，变成黑色。另外，**光线打在粗糙的物体上后，不再做弹射。**这就是为啥我们最终渲染出来的阴影是硬阴影。
>
>castRay函数中的菲涅尔函数是为了计算光线照射到一个物体后的反射与折射的比例。
